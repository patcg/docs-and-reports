<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <script src="https://www.w3.org/Tools/respec/respec-w3c" defer class="remove"></script>
  <title>Private Ad Technologies Principles</title>
  <script class="remove">
    var respecConfig = {
      specStatus: 'CG-DRAFT',
      group: 'patcg',
      editors: [
      ],
      github: 'patcg/docs-and-reports',
      shortName: 'pat-principles',
      latestVersion: null,
      localBiblio: {
        'Privacy-Principles': {
          title: 'Privacy Principles',
          href: 'https://www.w3.org/TR/privacy-principles/',
          authors: ['Robin Berjon', 'Jeffrey Yasskin'],
          publisher: 'W3C',
        },
      },
    };
  </script>
</head>
<body>

  <section id="abstract">
    <p>
      Privacy is essential to the sustainable success of the advertising ecosystem. This document takes up the
      W3C TAG's <em>Privacy Principles</em> [[?Privacy-Principles]] and specialises them for advertising-related
      situations.
    </p>
  </section>

  <section id="sotd">
    <p>
      This document is a draft of the Private Ad Technologies Community Group, it is intended to be contributed
      to an eventual Private Ad Technologies Working Group on the Note Track. This document does not yet
      reflect the consensus of the PATCG.
    </p>

  </section>

  <section class="introductory">
    <h2>How This Document Fits In</h2>
    <p>
      This document elaborates on the W3C TAG's <em>Privacy Principles</em> [[?Privacy-Principles]]. The latter
      document is intended to describe principles of privacy that apply across the Web, and therefore leaves the
      door open to a variety of approaches so that different use cases can be approached with some flexibility.
      This document is therefore more specific in detailing how the Web's broader privacy principles are to be
      understood in an advertising context.
    </p>
  </section>

  <section>
    <h2>Private Advertising Principles</h2>

    <p>Advertising-specific privacy principles may address the following issues:</p>

    <ul>
      <li>consent</li>
      <li>control</li>
      <li>profiling</li>
      <li>distress & intrusion</li> 
      <li>relevance</li>
      <li>reporting and context</li> 
      <li>transparency</li>
      <li>security</li>
      <li>trust</li>
      <li>explainability / comprehensibility</li> 
      <li>competition / consolidation / centralization</li>
      <li>inferences</li>
      <li>identifiers</li>
      <li>accountability</li>
    </ul>

    <p>Principles are organized in sections below regarding particular use cases or common concepts that apply across different use cases.</p>

    <section>
      <h3>Measurement</h3>

      <section>
        <h4>Measurement should be private for safe, widespread usage, but always be under user control</h4>
      </section>

      <section>
        <h4>Opting-out should not be visible</h4>
        <p>Users may wish to opt-out of participation in measurement, but do so in such a way that is not visible to the sites they visit. Visible opt-out could lead to retaliation against, or coercion of, users who do not wish to participate in measurement.</p>
      </section>

      <section>
        <h4>Measurement should not significantly enable cross-context recognition</h4>
        
        <p>Protections of differential privacy take the form of guarantees that aggregation or noise make participation in a particular measurement mostly indistinguishable, but also recognize that some information (as often quantified by parameters including epsilon) is released and could be combined with other known information to learn something with some (presumably very small) probability. "significantly" here is not yet detailed. The aggregated or noised measurement should not reasonably be usable to identify a particular user or to link an user's activity to another context.</p>

        <p class="note">
          Metrics to define significance are being evaluated by a separate task force.
        </p>

        <p>Because measurement and attribution involve all kinds of viewing advertisements and a variety of other actions, in a wide range of different contexts, relying on understanding of, expectations about and consent over cross-context recognition as a result of ad measurement would be inappropriate.</p>
      </section>

      <section>
        <h4>Measurement should not significantly enable inferences about individual people from their participation in the measurement</h4>

        <p>Related to cross-context recognition, measurement mechanisms should not reasonably be able to be used to learn or infer information about a particular user, for example, that a user visited a site (or class of site) or took an online or offline action.</p>

        <p>Population-level measurement can still be used for inference; this principle only indicates that participation (or non-participation) in the measurement cannot be used to enable an inference about that individual.</p>
      </section>
    </section>
    <section>
      <h3>Accountability</h3>
      <section>
        <h4>Users should be able to investigate how data about them is used and shared.</h4>

        <p>Users should be able to learn what measurements they may participate in.</p>
        <p>Users should be able to learn what level of risk of re-identification or cross-context data-sharing is possible. 
          <br><i>See also: comprehensibility.</i></p>
      </section>
      <section>
        <h4>Researchers, regulators and auditors should be able to investigate how a system is used and whether abuse is occurring.</h4>

        <p>Researchers should be able to learn what measurements are taking place, in order to identify unexpected or potentially abusive behavior and to explain the implications of the system to users (whose individual data may not be satisfyingly explanatory).</p>

        <p>Most users will not choose to investigate or be able to interpret individual data about measurements. Independent researchers can provide an important accountability function by identifying potentially significant or privacy-harmful outcomes.</p>

        <p>Some privacy harms -- including to small groups or vulnerable people -- cannot reasonably be identified in the individual case, but only with some aggregate analysis.</p>

        <p>Auditors, with internal access to at least one of the participating systems, should be able to investigate and document whether abuse has occurred (for example, collusion between non-colluding helper parties, or interfering with results). When evidence of abuse is discovered, affected parties must be notified.</p>
      </section>
      <section>
        <h4>When abuse happens, there must be a mechanism to identify the abuse, limit further access and enable consequences.</h4>
      </section>
    </section>
    <section>
      <h2>Leaks should be accounted for</h2>
      <p>
        Information flows should be accounted for. Unecessary data should not be collected, neither intentionally nor by mistake. 
        Designs and deployments must be audited for being data-leak proof. This can be achieved with technical, organisational and legal
        approaches, though legal-only approach may be insufficient.
      </p>

      <div class="issue">
        <p>
          Expand a little.
        </p>
      </div>
    </section>
  </section>

  <section class="appendix">
    <h2>Acknowledgements</h2>
    <p>
      The following people, in alphabetical order of their first name, were instrumental
      in producing this document:
    </p>
    <ul>
      <li>Robin Berjon</li>
    </ul>
  </section>
</body>
</html>
